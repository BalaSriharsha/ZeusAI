"""
Agent 3 -- Action Agent

Responsibilities:
  1. Receive messages from the other party (classified or raw)
  2. Use LLM to decide the appropriate action (speak, DTMF, wait, end call)
  3. Generate natural, dynamic speech responses
  4. All dialogue is LLM-generated -- nothing is hardcoded
"""

from __future__ import annotations
import json
import logging
from typing import Callable, Optional

from groq import Groq

from backend.models.schemas import (
    IVRClassification, IVRPromptType, UserIntent,
    AgentAction, ActionType, CallState,
)
from backend.config import settings

logger = logging.getLogger(__name__)

_client: Groq | None = None


def _get_client() -> Groq:
    global _client
    if _client is None:
        _client = Groq(api_key=settings.groq_api_key)
    return _client


AGENT_SYSTEM_PROMPT = """\
You are an AI phone agent making a call to {target_entity} on behalf of a user.
You are currently on the line with {target_entity}'s phone system.

--- USER'S REQUEST ---
{intent_summary}

--- USER DETAILS ---
Name: {user_name}
Phone: {user_phone}

--- HOW TO BEHAVE ---
1. When greeted or asked how you can be helped, clearly state the FULL request \
   in one natural sentence. Include all relevant details (action, specifics, \
   dates, names, etc.).
2. When asked for personal details (name, phone, etc.), provide them naturally. \
   Always include BOTH name and phone number together when identity is requested. \
   Spell out the phone number with pauses between digits for clarity. \
   Example: "My name is Bala and my mobile number is 9 3 0 4 5 6 6 3 3 6."
3. When asked to confirm details, respond naturally. Vary your phrasing. \
   Examples: "Yes, that is correct.", "That is right, thank you.", "Yes, all correct."
4. When a numbered DTMF menu is presented ("press 1 for X, press 2 for Y"), \
   choose the option that best matches the user's request and return dtmf action.
5. When told to hold or wait, or when hold music plays, return wait action.
6. When the conversation is ending (goodbye), respond with a polite farewell \
   and return end_call action.
7. Be natural and conversational -- vary your language, do not repeat the same \
   phrases every turn. Sound like a real human on a phone call.
8. When asked to enter a date via keypad, format it as digits (ddmmyyyy).
9. Answer ONLY what is asked. Do not volunteer extra information.

--- RESPONSE FORMAT ---
Return ONLY this JSON:
{{
  "action_type": "speak" | "dtmf" | "wait" | "end_call",
  "speech_text": "what you say out loud (null if dtmf or wait)",
  "dtmf_digits": "digit(s) to press (null if not dtmf)",
  "reasoning": "brief explanation"
}}

action_type rules:
- "speak"    = verbal response (the most common action)
- "dtmf"     = press key(s) on the phone keypad (only for numbered menus or date entry)
- "wait"     = stay silent (ONLY for hold music or explicit "please wait" messages)
- "end_call" = say goodbye and hang up (only at the very end of the call)\
"""


class ActionAgent:
    """
    Decides actions in response to the other party's messages.
    All responses are dynamically generated by the LLM.
    """

    def __init__(
        self,
        call_state: CallState,
        on_action: Callable[[AgentAction], None] | None = None,
    ):
        self.call_state = call_state
        self.on_action = on_action
        self._conversation_history: list[dict] = []

    async def handle_classification(
        self,
        classification: IVRClassification,
    ) -> AgentAction:
        """
        Receive a classified prompt and decide what to do.
        """
        logger.info(
            f"[Agent3] Handling {classification.prompt_type}: "
            f"{classification.raw_transcript[:80]}..."
        )

        self._conversation_history.append({
            "role": "other_party",
            "text": classification.raw_transcript,
        })

        action = await self._generate_action(
            classification.raw_transcript,
            classification,
        )

        self._log_action(action)

        if action.speech_text:
            self._conversation_history.append({
                "role": "agent",
                "text": action.speech_text,
            })

        if self.on_action:
            self.on_action(action)

        return action

    async def handle_raw_transcript(self, transcript: str) -> AgentAction:
        """
        Handle a raw transcript (from real call STT) without prior classification.
        Classifies internally then generates an action.
        """
        from backend.services import groq_llm

        logger.info(f"[Agent3] Raw transcript: {transcript[:80]}...")

        self._conversation_history.append({
            "role": "other_party",
            "text": transcript,
        })

        action = await self._generate_action(transcript)

        self._log_action(action)

        if action.speech_text:
            self._conversation_history.append({
                "role": "agent",
                "text": action.speech_text,
            })

        if self.on_action:
            self.on_action(action)

        return action

    async def _generate_action(
        self,
        other_party_text: str,
        classification: IVRClassification | None = None,
    ) -> AgentAction:
        """Use LLM to decide the action and generate natural speech."""
        intent = self.call_state.user_intent
        client = _get_client()

        target = intent.target_entity or "the other party"

        system = AGENT_SYSTEM_PROMPT.format(
            target_entity=target,
            intent_summary=self._build_intent_summary(intent),
            user_name=intent.user_name or "Unknown",
            user_phone=intent.user_phone or "Unknown",
        )

        messages = [{"role": "system", "content": system}]

        for turn in self._conversation_history:
            if turn["role"] == "other_party":
                messages.append({"role": "user", "content": turn["text"]})
            else:
                messages.append({"role": "assistant", "content": turn["text"]})

        if (
            not self._conversation_history
            or self._conversation_history[-1]["role"] != "other_party"
        ):
            messages.append({"role": "user", "content": other_party_text})

        try:
            response = client.chat.completions.create(
                model=settings.groq_llm_model,
                messages=messages,
                temperature=0.3,
                max_tokens=512,
                response_format={"type": "json_object"},
            )
            raw = response.choices[0].message.content.strip()
            result = json.loads(raw)

            action_type = {
                "speak": ActionType.SPEAK,
                "dtmf": ActionType.DTMF,
                "wait": ActionType.WAIT,
                "end_call": ActionType.END_CALL,
            }.get(result.get("action_type", "speak"), ActionType.SPEAK)

            return AgentAction(
                action_type=action_type,
                speech_text=result.get("speech_text"),
                dtmf_digits=result.get("dtmf_digits"),
                reasoning=result.get("reasoning", "LLM-generated response"),
            )

        except json.JSONDecodeError:
            logger.error(f"[Agent3] JSON parse failed: {raw[:200]}")
            return AgentAction(
                action_type=ActionType.SPEAK,
                speech_text="Could you please repeat that?",
                reasoning="JSON parse error, asking to repeat",
            )
        except Exception as e:
            logger.error(f"[Agent3] LLM call failed: {e}")
            return AgentAction(
                action_type=ActionType.SPEAK,
                speech_text="I am sorry, could you repeat that?",
                reasoning=f"LLM error: {str(e)}",
            )

    def _build_intent_summary(self, intent: UserIntent) -> str:
        """Build a human-readable summary of the user's intent."""
        parts = []

        # Use task_description if available, otherwise map from intent type
        if intent.task_description:
            parts.append(intent.task_description.capitalize())
        else:
            action_map = {
                "book_appointment": "Book a new appointment",
                "cancel_appointment": "Cancel an existing appointment",
                "reschedule_appointment": "Reschedule an appointment",
                "check_status": "Check appointment status",
                "general_inquiry": "Make a general inquiry",
                "complaint": "File a complaint",
                "phone_call": "Make a phone call",
            }
            parts.append(action_map.get(intent.intent.value, intent.intent.value))

        if intent.target_entity:
            parts.append(f"Target: {intent.target_entity}")
        if intent.doctor_specialty:
            parts.append(f"Specialty: {intent.doctor_specialty}")
        if intent.doctor_name:
            parts.append(f"Doctor: {intent.doctor_name}")
        if intent.hospital_name:
            hospital = intent.hospital_name
            if intent.hospital_branch:
                hospital += f", {intent.hospital_branch}"
            if intent.hospital_city:
                hospital += f", {intent.hospital_city}"
            parts.append(f"Hospital: {hospital}")
        if intent.appointment_date:
            parts.append(f"Date: {intent.appointment_date}")

        return ". ".join(parts)

    def _log_action(self, action: AgentAction) -> None:
        match action.action_type:
            case ActionType.SPEAK:
                logger.info(f"[Agent3] Speaking: {action.speech_text}")
            case ActionType.DTMF:
                logger.info(f"[Agent3] Pressing DTMF: {action.dtmf_digits}")
            case ActionType.END_CALL:
                logger.info(f"[Agent3] Ending call: {action.speech_text}")
            case ActionType.WAIT:
                logger.info(f"[Agent3] Waiting: {action.reasoning}")
